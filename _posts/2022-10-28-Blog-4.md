Blog 4 - Feature Selection

A raw dataset contains large nunber of features. Many of these features are not required as some does not make business sense (such as ID's) and some are explain
by other feature or group of features. Also, we perform transformation of variables and in such case a particular transformation works better than the original
feature, thus we remove that variable. There are different method of feature selection which are explained below:

The explanation is based on n and p where n is number of observations and p is number of varaibles.

1. Best subset selectiion
This method involves trying all most combination of variables. For p variables, we try 2^p combination. This can be restricted by specifying the max variable 
allowed. Say, maximum variable allowed is q where q<p. Then, the maximum possible coombination will be 2^q. We keep on doing this for all p or q variables and 
later compare AIC, BIC or djusted r-sqaure values. The one with smallest AIC, BIC values or highest r-square value is selected. Also, to avoid overfitting and 
simplify the model, if the variable addition causes marginal decrease in AIC, BIC values or marginal increase in adjusted r-squared values, then we don't 
consider those and prior one with less variable is selected.

2. Forward stepwise selection
In forward subset selection we first have a constant and on top of that we add variable which best describes the response. That is, after a constant is 
determinded, we add a variable from the available list of p variables (say x1). Next, we add a variable from the list of p-1 variable which along with the 
variable x1 (say x2). The best model is selected based on AIC, BIC and adjusted r-sqaure values explained in best subset selection. The drawback is all possible 
combination are not checked as in best subset selection.

3. Backward stepwise selection
The backward stepwise selection is reverse of forward stepwise selection. In this method, we first consider all the p variables and then we keep on removing one
at each step without which the response variable is best calculated. The best model is selected based on AIC, BIC and adjusted r-sqaure values explained in 
best subset selection. The drawback is all possible combination are not checked as in best subset selection.

4. Hybrid approach
This is a combination of forward and backward stepwise selection. In this method, we add new variable and check whethere there exist and variable which no longer 
provide improvement in the model fit. The best model is selected based on AIC, BIC and adjusted r-sqaure values explained in best subset selection. This method
is close to best subset selection as compared to forward and backward stepwise selection.

5. Lasso
Lasso is different from the above three methods mentioned. It involves a penalty factor lambda which is applied on addition of each variable. If addition of a
new variable explains the response less than the penalty term, it is removed or coeeficient of the variable becomes 0. In this way, the relevant features are 
selected and used in the model. This selection technique is highly driven by lambda which is determined using cross validation technique. 

Conslusion:
Among the method listed above, the selection method varies based on the dataset available. If the number of observation n is less than p or p is very large and 
there are computation problem , then I prefer forward subset selection method. If the number of variables are less, then best subset selection is preferred. 
Lasso involves tuning of penalty terms rather than a constant penalty term in AIC and BIC calculation which is an advantage. I would prefer to use more than
one method if possible.
